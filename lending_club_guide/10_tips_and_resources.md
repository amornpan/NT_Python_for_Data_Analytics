# ðŸ’¡ à¹€à¸„à¸¥à¹‡à¸”à¸¥à¸±à¸šà¹à¸¥à¸°à¹à¸«à¸¥à¹ˆà¸‡à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰

## ðŸ“‹ à¸ªà¸²à¸£à¸šà¸±à¸
- [ðŸŽ¯ à¹€à¸„à¸¥à¹‡à¸”à¸¥à¸±à¸šà¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰](#-à¹€à¸„à¸¥à¹‡à¸”à¸¥à¸±à¸šà¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰)
- [ðŸ› ï¸ Setup à¹à¸¥à¸° Environment](#ï¸-setup-à¹à¸¥à¸°-environment)
- [ðŸ“š à¹à¸«à¸¥à¹ˆà¸‡à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¹€à¸žà¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡](#-à¹à¸«à¸¥à¹ˆà¸‡à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¹€à¸žà¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡)
- [ðŸ”§ Tools à¹à¸¥à¸° Libraries](#-tools-à¹à¸¥à¸°-libraries)
- [ðŸ’¼ à¸à¸²à¸£à¹€à¸•à¸£à¸µà¸¢à¸¡à¸•à¸±à¸§à¸ªà¸³à¸«à¸£à¸±à¸šà¸‡à¸²à¸™](#-à¸à¸²à¸£à¹€à¸•à¸£à¸µà¸¢à¸¡à¸•à¸±à¸§à¸ªà¸³à¸«à¸£à¸±à¸šà¸‡à¸²à¸™)
- [â“ FAQ à¹à¸¥à¸° Troubleshooting](#-faq-à¹à¸¥à¸°-troubleshooting)

---

## ðŸŽ¯ à¹€à¸„à¸¥à¹‡à¸”à¸¥à¸±à¸šà¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰

### ðŸ“š **à¹€à¸„à¸¥à¹‡à¸”à¸¥à¸±à¸šà¸—à¸±à¹ˆà¸§à¹„à¸›**

#### **1. à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¸­à¸¢à¹ˆà¸²à¸‡à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡**
```python
# à¸ªà¸£à¹‰à¸²à¸‡à¸™à¸´à¸ªà¸±à¸¢à¸—à¸µà¹ˆà¸”à¸µà¸•à¸±à¹‰à¸‡à¹à¸•à¹ˆà¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² display options
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
plt.style.use('seaborn-v0_8')
```

#### **2. à¸à¸²à¸£à¸ˆà¸±à¸”à¸à¸²à¸£à¹„à¸Ÿà¸¥à¹Œà¹à¸¥à¸°à¹‚à¸›à¸£à¹€à¸ˆà¸„**
```
project_folder/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸”à¸´à¸š
â”‚   â”œâ”€â”€ processed/        # à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸”à¹à¸¥à¹‰à¸§
â”‚   â””â”€â”€ external/         # à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸à¸ à¸²à¸¢à¸™à¸­à¸
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_exploration.ipynb
â”‚   â”œâ”€â”€ 02_cleaning.ipynb
â”‚   â””â”€â”€ 03_modeling.ipynb
â”œâ”€â”€ src/                  # Python scripts
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ figures/
â”‚   â””â”€â”€ models/
â””â”€â”€ README.md
```

#### **3. à¸à¸²à¸£à¹€à¸‚à¸µà¸¢à¸™ Code à¸—à¸µà¹ˆà¸”à¸µ**
```python
# âŒ à¸«à¸¥à¸µà¸à¹€à¸¥à¸µà¹ˆà¸¢à¸‡
df2 = df.dropna()
result = df2.groupby('grade').mean()

# âœ… à¸„à¸§à¸£à¸—à¸³
df_clean = df.dropna()
grade_summary = df_clean.groupby('grade').agg({
    'loan_amnt': 'mean',
    'int_rate': 'mean',
    'annual_inc': 'mean'
}).round(2)
```

### ðŸ§  **à¸à¸¥à¸¢à¸¸à¸—à¸˜à¹Œà¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰**

#### **1. Learning by Doing**
- **80% Practice, 20% Theory**: à¹€à¸™à¹‰à¸™à¸à¸²à¸£à¸›à¸à¸´à¸šà¸±à¸•à¸´à¸¡à¸²à¸à¸à¸§à¹ˆà¸²à¸à¸²à¸£à¸­à¹ˆà¸²à¸™
- **Project-Based Learning**: à¸—à¸³ project à¸ˆà¸£à¸´à¸‡à¸•à¸±à¹‰à¸‡à¹à¸•à¹ˆà¹€à¸£à¸´à¹ˆà¸¡à¹€à¸£à¸µà¸¢à¸™
- **Iterative Improvement**: à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡à¹‚à¸„à¹‰à¸”à¹à¸¥à¸°à¸œà¸¥à¸‡à¸²à¸™à¸­à¸¢à¹ˆà¸²à¸‡à¸•à¹ˆà¸­à¹€à¸™à¸·à¹ˆà¸­à¸‡

#### **2. Spaced Repetition**
```python
# à¸ªà¸±à¸›à¸”à¸²à¸«à¹Œà¸—à¸µà¹ˆ 1: à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰ pandas basics
# à¸ªà¸±à¸›à¸”à¸²à¸«à¹Œà¸—à¸µà¹ˆ 2: à¸—à¸šà¸—à¸§à¸™ pandas + à¹€à¸£à¸µà¸¢à¸™ visualization
# à¸ªà¸±à¸›à¸”à¸²à¸«à¹Œà¸—à¸µà¹ˆ 3: à¸—à¸šà¸—à¸§à¸™à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸” + à¹€à¸£à¸µà¸¢à¸™ ML
# à¸ªà¸±à¸›à¸”à¸²à¸«à¹Œà¸—à¸µà¹ˆ 4: à¸—à¸šà¸—à¸§à¸™à¹à¸¥à¸°à¸—à¸³ project
```

#### **3. à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¸ˆà¸²à¸à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”**
```python
# à¹€à¸à¹‡à¸š error log à¹à¸¥à¸°à¸§à¸´à¸˜à¸µà¹à¸à¹‰à¹„à¸‚
common_errors = {
    'KeyError': 'à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸Šà¸·à¹ˆà¸­ column à¹ƒà¸«à¹‰à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡',
    'ValueError': 'à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š data type à¹à¸¥à¸° missing values',
    'IndexError': 'à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¸™à¸²à¸”à¸‚à¸­à¸‡ array/list',
    'MemoryError': 'à¸¥à¸” chunk size à¸«à¸£à¸·à¸­ sample data'
}
```

### ðŸ“ **à¸à¸²à¸£à¸ˆà¸”à¸šà¸±à¸™à¸—à¸¶à¸à¸—à¸µà¹ˆà¸¡à¸µà¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸ž**

#### **Jupyter Notebook Best Practices**
```python
"""
# ðŸ“Š Lending Club Data Analysis
## Objective: Understand default patterns

### Key Questions:
1. What factors predict default?
2. How does grade relate to risk?
3. What is the optimal portfolio mix?
"""

# Cell 1: Data Loading
import pandas as pd
df = pd.read_csv('data.csv')
print(f"Data shape: {df.shape}")

# Cell 2: Basic Exploration  
df.head()
```

#### **à¸à¸²à¸£ Comment à¹à¸¥à¸° Documentation**
```python
def calculate_default_rate(df, group_by_col):
    """
    à¸„à¸³à¸™à¸§à¸“à¸­à¸±à¸•à¸£à¸²à¸à¸²à¸£à¸œà¸´à¸”à¸™à¸±à¸”à¸Šà¸³à¸£à¸°à¸•à¸²à¸¡à¸à¸¥à¸¸à¹ˆà¸¡
    
    Parameters:
    -----------
    df : pandas.DataFrame
        à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸´à¸™à¹€à¸Šà¸·à¹ˆà¸­
    group_by_col : str
        à¸Šà¸·à¹ˆà¸­à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸ˆà¸±à¸”à¸à¸¥à¸¸à¹ˆà¸¡
    
    Returns:
    --------
    pandas.Series
        à¸­à¸±à¸•à¸£à¸²à¸à¸²à¸£à¸œà¸´à¸”à¸™à¸±à¸”à¸Šà¸³à¸£à¸°à¸‚à¸­à¸‡à¹à¸•à¹ˆà¸¥à¸°à¸à¸¥à¸¸à¹ˆà¸¡
    
    Example:
    --------
    >>> default_rates = calculate_default_rate(df, 'grade')
    >>> print(default_rates)
    """
    return df.groupby(group_by_col)['is_default'].mean()
```

---

## ðŸ› ï¸ Setup à¹à¸¥à¸° Environment

### ðŸ **Python Environment Setup**

#### **1. Anaconda Installation**
```bash
# Download Anaconda from https://www.anaconda.com/
# à¸«à¸£à¸·à¸­à¹ƒà¸Šà¹‰ Miniconda à¸ªà¸³à¸«à¸£à¸±à¸š lightweight version

# à¸ªà¸£à¹‰à¸²à¸‡ environment à¸ªà¸³à¸«à¸£à¸±à¸š project
conda create -n lending_analysis python=3.9
conda activate lending_analysis

# à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ packages
conda install pandas numpy matplotlib seaborn scikit-learn jupyter
pip install plotly dash streamlit
```

#### **2. Virtual Environment (Alternative)**
```bash
# à¸ªà¸£à¹‰à¸²à¸‡ virtual environment
python -m venv lending_env

# Activate (Windows)
lending_env\Scripts\activate

# Activate (Mac/Linux)  
source lending_env/bin/activate

# à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ packages
pip install -r requirements.txt
```

#### **3. Requirements.txt**
```txt
pandas>=1.5.0
numpy>=1.24.0
matplotlib>=3.6.0
seaborn>=0.12.0
scikit-learn>=1.2.0
scipy>=1.10.0
jupyter>=1.0.0
plotly>=5.15.0
streamlit>=1.25.0
```

### ðŸ’» **Development Tools**

#### **IDE à¹à¸¥à¸° Editors**
| Tool | à¸‚à¹‰à¸­à¸”à¸µ | à¹€à¸«à¸¡à¸²à¸°à¸ªà¸³à¸«à¸£à¸±à¸š |
|------|-------|------------|
| **Jupyter Notebook** | Interactive, visualization | Exploration, prototyping |
| **VS Code** | Versatile, extensions | Development, production code |
| **PyCharm** | Full-featured IDE | Large projects, debugging |
| **Google Colab** | Cloud-based, free GPU | Quick experiments, sharing |

#### **Jupyter Extensions à¸—à¸µà¹ˆà¹à¸™à¸°à¸™à¸³**
```bash
# Jupyter Lab extensions
pip install jupyterlab
jupyter labextension install @jupyter-widgets/jupyterlab-manager

# Useful extensions
pip install jupyter_contrib_nbextensions
jupyter contrib nbextension install --user
```

### â˜ï¸ **Cloud Platforms**

#### **Google Colab Setup**
```python
# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Install additional packages
!pip install seaborn plotly

# Load data from Drive
import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/data/lending_club.csv')
```

#### **Kaggle Notebooks**
```python
# Access Kaggle datasets
import kaggle
from kaggle.api.kaggle_api_extended import KaggleApi

# Setup kaggle credentials
api = KaggleApi()
api.authenticate()

# Download dataset
api.dataset_download_files('dataset-name', path='./data', unzip=True)
```

---

## ðŸ“š à¹à¸«à¸¥à¹ˆà¸‡à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¹€à¸žà¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡

### ðŸ“– **à¸«à¸™à¸±à¸‡à¸ªà¸·à¸­à¹à¸™à¸°à¸™à¸³**

#### **à¸ªà¸³à¸«à¸£à¸±à¸šà¸œà¸¹à¹‰à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™**
1. **"Python for Data Analysis" by Wes McKinney**
   - à¸œà¸¹à¹‰à¸ªà¸£à¹‰à¸²à¸‡ pandas library
   - à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸„à¸£à¸­à¸šà¸„à¸¥à¸¸à¸¡à¹à¸¥à¸°à¸›à¸à¸´à¸šà¸±à¸•à¸´à¹„à¸”à¹‰à¸ˆà¸£à¸´à¸‡
   - à¸¡à¸µ hands-on examples à¸¡à¸²à¸à¸¡à¸²à¸¢

2. **"Hands-On Machine Learning" by AurÃ©lien GÃ©ron**
   - à¹€à¸«à¸¡à¸²à¸°à¸ªà¸³à¸«à¸£à¸±à¸šà¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™ ML
   - à¸¡à¸µ practical projects
   - à¸­à¸˜à¸´à¸šà¸²à¸¢à¹à¸™à¸§à¸„à¸´à¸”à¹„à¸”à¹‰à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆà¸‡à¹ˆà¸²à¸¢

#### **à¸ªà¸³à¸«à¸£à¸±à¸šà¸£à¸°à¸”à¸±à¸šà¸à¸¥à¸²à¸‡-à¸ªà¸¹à¸‡**
3. **"The Elements of Statistical Learning" by Hastie, Tibshirani, Friedman**
   - à¸—à¸¤à¸©à¸Žà¸µ ML à¸—à¸µà¹ˆà¸¥à¸¶à¸à¸‹à¸¶à¹‰à¸‡
   - Mathematical foundations
   - Reference book à¸ªà¸³à¸«à¸£à¸±à¸š practitioners

4. **"Storytelling with Data" by Cole Nussbaumer Knaflic**
   - Data visualization à¹à¸¥à¸° communication
   - Business storytelling
   - Practical design principles

### ðŸŒ **Online Courses**

#### **Free Resources**
| Platform | Course | à¸£à¸°à¸”à¸±à¸š | à¹€à¸§à¸¥à¸² |
|----------|--------|-------|------|
| **Coursera** | Python for Everybody | Beginner | 8 weeks |
| **edX** | Introduction to Data Science | Intermediate | 12 weeks |
| **Kaggle Learn** | Pandas, ML courses | All levels | 2-4 hours each |
| **YouTube** | Corey Schafer's Python tutorials | All levels | Variable |

#### **Paid Courses**
| Platform | Course | Price Range | Value |
|----------|--------|-------------|-------|
| **DataCamp** | Data Science tracks | $25-35/month | Interactive |
| **Pluralsight** | Python/ML paths | $29-45/month | Comprehensive |
| **Udemy** | Specific courses | $10-200 each | Project-based |

### ðŸ“º **Video Learning**

#### **YouTube Channels à¹à¸™à¸°à¸™à¸³**
1. **3Blue1Brown** - Mathematical concepts visualization
2. **Corey Schafer** - Python programming tutorials
3. **Data School** - pandas à¹à¸¥à¸° ML tutorials
4. **StatQuest** - Statistics à¹à¸¥à¸° ML concepts
5. **Two Minute Papers** - Latest AI/ML research

#### **Podcasts**
1. **Data Skeptic** - Data science topics
2. **Linear Digressions** - ML à¹à¸¥à¸° statistics
3. **Towards Data Science Podcast** - Industry insights
4. **The AI Podcast** - AI trends à¹à¸¥à¸° applications

### ðŸ“Š **Practice Platforms**

#### **Kaggle**
```python
# Kaggle competitions à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸¶à¸à¸à¸™
beginner_competitions = [
    'House Prices: Advanced Regression Techniques',
    'Titanic: Machine Learning from Disaster', 
    'Digit Recognizer',
    'Bike Sharing Demand'
]

# Kaggle Datasets à¸ªà¸³à¸«à¸£à¸±à¸šà¹‚à¸›à¸£à¹€à¸ˆà¸„
interesting_datasets = [
    'Credit Card Fraud Detection',
    'Customer Churn Prediction',
    'Stock Market Data',
    'E-commerce Customer Behavior'
]
```

#### **HackerRank & LeetCode**
- Programming challenges
- Algorithm practice
- Interview preparation

---

## ðŸ”§ Tools à¹à¸¥à¸° Libraries

### ðŸ“Š **Data Manipulation & Analysis**

#### **Pandas Advanced Tips**
```python
# Memory optimization
def optimize_memory(df):
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type != 'object':
            c_min = df[col].min()
            c_max = df[col].max()
            
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                    
    return df

# Chaining operations
result = (df
    .query('loan_amnt > 5000')
    .groupby('grade')
    .agg({'annual_inc': 'mean', 'loan_amnt': 'count'})
    .round(2)
    .reset_index()
)
```

#### **NumPy Performance Tips**
```python
# Vectorized operations
# âŒ Slow
result = []
for i in range(len(arr)):
    result.append(arr[i] * 2)

# âœ… Fast  
result = arr * 2

# Boolean indexing
# Filter data efficiently
high_income = df[df['annual_inc'] > 100000]
grade_a_b = df[df['grade'].isin(['A', 'B'])]
```

### ðŸ“ˆ **Visualization Libraries**

#### **Matplotlib Customization**
```python
# Custom plotting style
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 12

# Create professional plots
def create_professional_plot(data, title):
    fig, ax = plt.subplots(figsize=(12, 8))
    
    # Main plot
    ax.plot(data.index, data.values, linewidth=2.5, color='#2E86C1')
    
    # Styling
    ax.set_title(title, fontsize=16, fontweight='bold', pad=20)
    ax.set_xlabel('X Label', fontsize=14)
    ax.set_ylabel('Y Label', fontsize=14)
    
    # Remove spines
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    
    # Grid
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig, ax
```

#### **Seaborn Advanced Usage**
```python
# Custom color palettes
custom_palette = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']
sns.set_palette(custom_palette)

# Multi-plot figures
def create_analysis_dashboard(df):
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # Distribution plot
    sns.histplot(data=df, x='loan_amnt', hue='grade', ax=axes[0,0])
    axes[0,0].set_title('Loan Amount Distribution by Grade')
    
    # Correlation heatmap
    corr_matrix = df.select_dtypes(include=[np.number]).corr()
    sns.heatmap(corr_matrix, annot=True, ax=axes[0,1], cmap='coolwarm')
    axes[0,1].set_title('Correlation Matrix')
    
    # Box plot
    sns.boxplot(data=df, x='grade', y='int_rate', ax=axes[1,0])
    axes[1,0].set_title('Interest Rate by Grade')
    
    # Scatter plot
    sns.scatterplot(data=df, x='annual_inc', y='loan_amnt', 
                   hue='grade', ax=axes[1,1])
    axes[1,1].set_title('Income vs Loan Amount')
    
    plt.tight_layout()
    return fig
```

### ðŸ¤– **Machine Learning Libraries**

#### **Scikit-learn Pipeline**
```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Complete ML pipeline
def create_ml_pipeline():
    # Preprocessing for numerical features
    numerical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    
    # Preprocessing for categorical features
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])
    
    # Combine preprocessing steps
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numerical_transformer, numerical_features),
            ('cat', categorical_transformer, categorical_features)
        ]
    )
    
    # Create full pipeline
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', RandomForestClassifier(random_state=42))
    ])
    
    return pipeline
```

### ðŸ”¬ **Specialized Tools**

#### **Financial Analysis Libraries**
```python
# yfinance for stock data
import yfinance as yf
stock_data = yf.download('AAPL', start='2020-01-01', end='2023-01-01')

# QuantLib for financial calculations
import QuantLib as ql
# Advanced financial modeling

# pandas-ta for technical indicators
import pandas_ta as ta
df.ta.sma(length=20)  # Simple moving average
```

#### **Statistical Libraries**
```python
# Statsmodels for statistical analysis
import statsmodels.api as sm
from statsmodels.stats.proportion import proportions_ztest

# Scipy for scientific computing
from scipy import stats
from scipy.optimize import minimize

# Pingouin for statistical tests
import pingouin as pg
pg.ttest(x, y)  # t-test with detailed output
```

---

## ðŸ’¼ à¸à¸²à¸£à¹€à¸•à¸£à¸µà¸¢à¸¡à¸•à¸±à¸§à¸ªà¸³à¸«à¸£à¸±à¸šà¸‡à¸²à¸™

### ðŸ“ **à¸ªà¸£à¹‰à¸²à¸‡ Portfolio**

#### **GitHub Portfolio Structure**
```
your-github/
â”œâ”€â”€ data-science-portfolio/
â”‚   â”œâ”€â”€ README.md                 # Portfolio overview
â”‚   â”œâ”€â”€ lending-club-analysis/    # This project
â”‚   â”œâ”€â”€ customer-churn-prediction/
â”‚   â”œâ”€â”€ stock-price-analysis/
â”‚   â””â”€â”€ ab-testing-analysis/
â”œâ”€â”€ contributions/
â”‚   â””â”€â”€ open-source-projects/
â””â”€â”€ learning-notes/
    â”œâ”€â”€ pandas-cheatsheet.md
    â”œâ”€â”€ ml-algorithms-summary.md
    â””â”€â”€ sql-reference.md
```

#### **Project Documentation Template**
```markdown
# Project Title

## Business Problem
Clear problem statement and context

## Data
- Source: Where did the data come from?
- Size: How much data?
- Features: Key variables

## Methodology
- Data cleaning approach
- Analysis techniques used
- Models implemented

## Key Findings
- 3-5 bullet points of insights
- Business implications

## Technical Skills Demonstrated
- Python libraries used
- Statistical techniques
- ML algorithms

## Files
- `notebooks/`: Jupyter notebooks
- `src/`: Python scripts  
- `data/`: Sample data (if shareable)
- `outputs/`: Visualizations and results
```

### ðŸŽ¯ **Resume Tips**

#### **Technical Skills Section**
```markdown
**Programming Languages**: Python, SQL, R
**Data Analysis**: pandas, NumPy, SciPy, statsmodels
**Machine Learning**: scikit-learn, XGBoost, TensorFlow
**Visualization**: matplotlib, seaborn, Plotly, Tableau
**Tools**: Jupyter, Git, Docker, AWS, Google Cloud
**Databases**: PostgreSQL, MySQL, MongoDB
```

#### **Project Descriptions**
```markdown
**Credit Risk Analysis | Lending Club Dataset**
- Analyzed 145K+ loan records to predict default probability using Random Forest and Logistic Regression
- Achieved 0.85 AUC score, identifying key risk factors (FICO score, DTI ratio, loan grade)
- Developed automated credit scoring system with business rules, potentially reducing default rate by 15%
- Technologies: Python, pandas, scikit-learn, seaborn
```

### ðŸ¤ **Interview Preparation**

#### **Technical Interview Topics**
```python
# Common coding challenges
def interview_prep_topics():
    topics = {
        'Data Manipulation': [
            'pandas groupby operations',
            'Handling missing data',
            'Data type conversions',
            'Merging/joining datasets'
        ],
        'Statistics': [
            'Hypothesis testing',
            'Correlation vs causation', 
            'Bias and variance',
            'Statistical significance'
        ],
        'Machine Learning': [
            'Train/validation/test splits',
            'Overfitting prevention',
            'Feature selection/engineering',
            'Model evaluation metrics'
        ],
        'Business Cases': [
            'A/B testing design',
            'Recommendation systems',
            'Churn prediction',
            'Price optimization'
        ]
    }
    return topics
```

#### **Mock Interview Questions**
1. **Technical**: "How would you handle missing values in this dataset?"
2. **Business**: "How would you measure the success of a recommendation system?"
3. **Coding**: "Write a function to calculate ROI for each customer segment"
4. **Case Study**: "Design an experiment to test a new pricing strategy"

### ðŸ” **Job Search Strategy**

#### **Target Companies by Type**
```python
company_types = {
    'Tech Giants': ['Google', 'Meta', 'Amazon', 'Microsoft'],
    'Fintech': ['Stripe', 'Square', 'PayPal', 'Robinhood'],
    'Traditional Finance': ['JPMorgan', 'Goldman Sachs', 'Citi'],
    'Consulting': ['McKinsey', 'BCG', 'Deloitte'],
    'Startups': ['Y Combinator companies', 'Series A-C companies']
}

# Match your skills to company needs
skill_company_match = {
    'Financial Analytics': ['Banks', 'Fintech', 'Insurance'],
    'Marketing Analytics': ['E-commerce', 'Social Media', 'AdTech'],
    'Product Analytics': ['Tech companies', 'SaaS', 'Mobile apps'],
    'Operations Analytics': ['Logistics', 'Manufacturing', 'Healthcare']
}
```

#### **Networking Tips**
- **LinkedIn**: Connect with data scientists, comment on posts
- **Meetups**: Attend local data science meetups
- **Conferences**: DataCon, Strata, local conferences
- **Online Communities**: Kaggle, GitHub, Reddit r/MachineLearning

---

## â“ FAQ à¹à¸¥à¸° Troubleshooting

### ðŸ› **Common Errors**

#### **Data Loading Issues**
```python
# Error: UnicodeDecodeError
# Solution: Try different encodings
try:
    df = pd.read_csv('data.csv', encoding='utf-8')
except UnicodeDecodeError:
    df = pd.read_csv('data.csv', encoding='latin-1')

# Error: Memory issues with large files
# Solution: Read in chunks
def read_large_csv(filename, chunksize=10000):
    chunks = []
    for chunk in pd.read_csv(filename, chunksize=chunksize):
        # Process each chunk
        processed_chunk = chunk.dropna()
        chunks.append(processed_chunk)
    return pd.concat(chunks, ignore_index=True)
```

#### **Pandas Operations**
```python
# Error: KeyError when accessing columns
# Solution: Check column names
print(df.columns.tolist())  # See all column names
df.columns = df.columns.str.strip()  # Remove whitespace

# Error: SettingWithCopyWarning
# Solution: Use .copy() or .loc properly
df_clean = df.copy()  # Create explicit copy
df.loc[df['grade'] == 'A', 'risk_level'] = 'Low'  # Proper indexing
```

#### **Machine Learning Issues**
```python
# Error: Target variable leakage
# Solution: Check features carefully
def check_data_leakage(X, y):
    # Features that are too predictive might be leakage
    from sklearn.ensemble import RandomForestClassifier
    rf = RandomForestClassifier(n_estimators=10)
    rf.fit(X, y)
    
    # Check for suspiciously high importance
    importance = pd.Series(rf.feature_importances_, index=X.columns)
    suspicious = importance[importance > 0.8]
    
    if len(suspicious) > 0:
        print("Potential data leakage in features:", suspicious.index.tolist())

# Error: Poor model performance
# Solution: Debug systematically
def debug_model_performance(X, y, model):
    # Check data distribution
    print("Target distribution:", y.value_counts())
    
    # Check for missing values
    print("Missing values:", X.isnull().sum().sum())
    
    # Check feature correlation
    high_corr = X.corr().abs().unstack().sort_values(ascending=False)
    high_corr = high_corr[high_corr < 1.0]
    print("Highly correlated features:", high_corr.head())
```

### ðŸ’¡ **Performance Optimization**

#### **Pandas Optimization**
```python
# Use categorical data for string columns with few unique values
df['grade'] = df['grade'].astype('category')

# Use vectorized operations instead of loops
# âŒ Slow
df['new_col'] = df.apply(lambda x: x['col1'] * x['col2'], axis=1)

# âœ… Fast
df['new_col'] = df['col1'] * df['col2']

# Use query() for complex filtering
# âŒ Slower
result = df[(df['grade'].isin(['A', 'B'])) & (df['loan_amnt'] > 10000)]

# âœ… Faster
result = df.query("grade in ['A', 'B'] and loan_amnt > 10000")
```

#### **Memory Management**
```python
# Monitor memory usage
def check_memory_usage(df):
    return df.memory_usage(deep=True).sum() / 1024**2  # MB

# Reduce memory with appropriate dtypes
def optimize_dtypes(df):
    for col in df.select_dtypes(include=['int64']):
        max_val = df[col].max()
        min_val = df[col].min()
        
        if min_val >= 0:  # Unsigned integers
            if max_val < 255:
                df[col] = df[col].astype('uint8')
            elif max_val < 65535:
                df[col] = df[col].astype('uint16')
        else:  # Signed integers
            if min_val > -128 and max_val < 127:
                df[col] = df[col].astype('int8')
            elif min_val > -32768 and max_val < 32767:
                df[col] = df[col].astype('int16')
    
    return df
```

### ðŸ”§ **Environment Issues**

#### **Package Installation Problems**
```bash
# Conda environment conflicts
conda clean --all
conda update conda
conda create -n fresh_env python=3.9
conda activate fresh_env

# Pip installation issues
pip install --upgrade pip
pip install --no-cache-dir package_name

# Jupyter kernel issues
python -m ipykernel install --user --name=lending_analysis
```

#### **Import Errors**
```python
# Check if package is installed
import sys
print(sys.path)

# Install from within Jupyter
import subprocess
subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'package_name'])

# Alternative import handling
try:
    import seaborn as sns
except ImportError:
    print("Seaborn not installed. Install with: pip install seaborn")
    sys.exit(1)
```

### ðŸ“ž **Getting Help**

#### **Online Communities**
1. **Stack Overflow**: Programming questions with tags [pandas], [python], [machine-learning]
2. **Reddit**: r/MachineLearning, r/datascience, r/Python
3. **Discord/Slack**: Data science communities
4. **GitHub Discussions**: Library-specific questions

#### **Documentation Resources**
```python
# Built-in help
help(pd.read_csv)
pd.read_csv?  # In Jupyter

# Online documentation
resources = {
    'pandas': 'https://pandas.pydata.org/docs/',
    'scikit-learn': 'https://scikit-learn.org/stable/',
    'matplotlib': 'https://matplotlib.org/stable/',
    'seaborn': 'https://seaborn.pydata.org/',
    'numpy': 'https://numpy.org/doc/'
}
```

---

## ðŸŽ“ **à¸šà¸—à¸ªà¸£à¸¸à¸›**

à¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰ Data Analytics à¹€à¸›à¹‡à¸™à¸à¸²à¸£à¹€à¸”à¸´à¸™à¸—à¸²à¸‡à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¹ƒà¸Šà¹‰à¹€à¸§à¸¥à¸²à¹à¸¥à¸°à¸„à¸§à¸²à¸¡à¸žà¸¢à¸²à¸¢à¸²à¸¡ à¹à¸•à¹ˆà¸”à¹‰à¸§à¸¢à¹€à¸„à¸¥à¹‡à¸”à¸¥à¸±à¸šà¹à¸¥à¸°à¹à¸«à¸¥à¹ˆà¸‡à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡ à¸„à¸¸à¸“à¸ªà¸²à¸¡à¸²à¸£à¸–à¸žà¸±à¸’à¸™à¸²à¸—à¸±à¸à¸©à¸°à¹à¸¥à¸°à¹€à¸›à¹‡à¸™à¸œà¸¹à¹‰à¹€à¸Šà¸µà¹ˆà¸¢à¸§à¸Šà¸²à¸à¹„à¸”à¹‰

### ðŸŒŸ **à¸«à¸¥à¸±à¸à¸à¸²à¸£à¸ªà¸³à¸„à¸±à¸à¸—à¸µà¹ˆà¸„à¸§à¸£à¸ˆà¸³**

âœ… **Practice Makes Perfect**: à¸à¸¶à¸à¸à¸™à¸­à¸¢à¹ˆà¸²à¸‡à¸•à¹ˆà¸­à¹€à¸™à¸·à¹ˆà¸­à¸‡  
âœ… **Learn by Doing**: à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¸œà¹ˆà¸²à¸™à¸à¸²à¸£à¸›à¸à¸´à¸šà¸±à¸•à¸´à¸ˆà¸£à¸´à¸‡  
âœ… **Stay Updated**: à¸•à¸´à¸”à¸•à¸²à¸¡à¹€à¸—à¸„à¹‚à¸™à¹‚à¸¥à¸¢à¸µà¹ƒà¸«à¸¡à¹ˆà¹†  
âœ… **Build Portfolio**: à¸ªà¸£à¹‰à¸²à¸‡à¸œà¸¥à¸‡à¸²à¸™à¸—à¸µà¹ˆà¹à¸ªà¸”à¸‡à¸„à¸§à¸²à¸¡à¸ªà¸²à¸¡à¸²à¸£à¸–  
âœ… **Network Actively**: à¸ªà¸£à¹‰à¸²à¸‡à¹€à¸„à¸£à¸·à¸­à¸‚à¹ˆà¸²à¸¢à¹ƒà¸™à¸§à¸‡à¸à¸²à¸£  

### ðŸš€ **à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸•à¹ˆà¸­à¹„à¸›**

1. **à¸—à¸³à¹‚à¸›à¸£à¹€à¸ˆà¸„à¸ªà¹ˆà¸§à¸™à¸•à¸±à¸§**: à¹ƒà¸Šà¹‰à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸ªà¸™à¹ƒà¸ˆ
2. **à¹€à¸‚à¹‰à¸²à¸£à¹ˆà¸§à¸¡à¸Šà¸¸à¸¡à¸Šà¸™**: à¸«à¸²à¹€à¸žà¸·à¹ˆà¸­à¸™à¸£à¹ˆà¸§à¸¡à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰
3. **à¹à¸Šà¸£à¹Œà¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰**: à¹€à¸‚à¸µà¸¢à¸™ blog à¸«à¸£à¸·à¸­à¸ªà¸­à¸™à¸„à¸™à¸­à¸·à¹ˆà¸™
4. **à¸ªà¸¡à¸±à¸„à¸£à¸‡à¸²à¸™**: à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¸à¸²à¸£à¸—à¸³à¸‡à¸²à¸™à¹ƒà¸™à¸ªà¸²à¸¢ Data

**à¸‚à¸­à¹ƒà¸«à¹‰à¹‚à¸Šà¸„à¸”à¸µà¹ƒà¸™à¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¹à¸¥à¸°à¸à¸²à¸£à¸›à¸£à¸°à¸¢à¸¸à¸à¸•à¹Œà¹ƒà¸Šà¹‰ Data Analytics! ðŸŽ‰**

---

*ðŸ“– à¸«à¸¡à¸²à¸¢à¹€à¸«à¸•à¸¸: à¹€à¸„à¸¥à¹‡à¸”à¸¥à¸±à¸šà¹à¸¥à¸°à¹à¸«à¸¥à¹ˆà¸‡à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¹€à¸«à¸¥à¹ˆà¸²à¸™à¸µà¹‰à¸£à¸§à¸šà¸£à¸§à¸¡à¸ˆà¸²à¸à¸›à¸£à¸°à¸ªà¸šà¸à¸²à¸£à¸“à¹Œà¸ˆà¸£à¸´à¸‡à¹à¸¥à¸° best practices à¹ƒà¸™à¸§à¸‡à¸à¸²à¸£ Data Science à¸ªà¸²à¸¡à¸²à¸£à¸–à¸›à¸£à¸±à¸šà¹ƒà¸Šà¹‰à¸•à¸²à¸¡à¸„à¸§à¸²à¸¡à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¹à¸¥à¸°à¸£à¸°à¸”à¸±à¸šà¸„à¸§à¸²à¸¡à¸ªà¸²à¸¡à¸²à¸£à¸–à¸‚à¸­à¸‡à¹à¸•à¹ˆà¸¥à¸°à¸šà¸¸à¸„à¸„à¸¥*

---
[â¬…ï¸ à¸à¸¥à¸±à¸šà¹„à¸›à¸—à¸µà¹ˆ Learning Outcomes](./09_learning_outcomes.md) | [ðŸ  à¸à¸¥à¸±à¸šà¹„à¸›à¸—à¸µà¹ˆ README](./README.md)
