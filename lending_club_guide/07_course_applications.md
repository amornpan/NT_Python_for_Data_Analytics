# üéì ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£

## üìã ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç
- [üìö ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 1: Essential Data Analytics](#-‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà-1-essential-data-analytics)
- [üìä ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 2: Data Profiling & Preparation](#-‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà-2-data-profiling--preparation)
- [ü§ñ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 3: Predictive Analytics & ML](#-‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà-3-predictive-analytics--ml)
- [üîó ‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ß‡∏±‡∏ô](#-‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ß‡∏±‡∏ô)
- [üí° Tips ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡∏™‡∏≠‡∏ô](#-tips-‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡∏™‡∏≠‡∏ô)

---

## üìö ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 1: Essential Data Analytics
*‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á insights ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô*

### üéØ **Learning Objectives**
- ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Lending Club
- ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ pandas ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏£‡∏ß‡∏à‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
- ‡∏™‡∏£‡πâ‡∏≤‡∏á basic visualizations ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£ insights
- ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à business context ‡∏Ç‡∏≠‡∏á P2P lending

### üìä **Morning Session (3 hours)**

#### **Hour 1: Data Introduction & Setup**
```python
# 1.1 Loading ‡πÅ‡∏•‡∏∞ Basic Exploration
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv('LoanStats_web_14422.csv')

print("=== BASIC DATA EXPLORATION ===")
print(f"Dataset shape: {df.shape}")
print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
print(df.head())
```

#### **Hour 2: Business Growth Analysis**
```python
# 1.2 ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï‡∏Ç‡∏≠‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤
df['issue_d'] = pd.to_datetime(df['issue_d'])
df['issue_year'] = df['issue_d'].dt.year

yearly_summary = df.groupby('issue_year').agg({
    'loan_amnt': ['count', 'sum', 'mean'],
    'int_rate': 'mean'
}).round(2)

print("=== BUSINESS GROWTH ANALYSIS ===")
print(yearly_summary)
```

#### **Hour 3: Risk Grade Analysis**
```python
# 1.3 ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≤‡∏° Grade
grade_analysis = df.groupby('grade').agg({
    'loan_amnt': ['count', 'sum', 'mean'],
    'int_rate': 'mean',
    'annual_inc': 'mean'
}).round(2)

print("=== GRADE ANALYSIS ===")
print(grade_analysis)
```

### üïê **Afternoon Session (3 hours)**

#### **Hour 4-6: Geographic & Purpose Analysis**
```python
# 1.4 ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏†‡∏π‡∏°‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå
top_states = df['addr_state'].value_counts().head(10)
purpose_analysis = df['purpose'].value_counts()

print("=== GEOGRAPHIC & PURPOSE ANALYSIS ===")
print(top_states)
print(purpose_analysis)
```

### üìù **Day 1 Deliverables**
1. **Data Exploration Report**: ‡∏™‡∏£‡∏∏‡∏õ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
2. **Business Growth Analysis**: ‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏•‡∏∞ insights ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï
3. **Customer Profile Summary**: ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Ç‡∏≠‡∏á‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞ grade

---

## üìä ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 2: Data Profiling & Preparation
*‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á features*

### üéØ **Learning Objectives**
- ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
- ‡∏™‡∏£‡πâ‡∏≤‡∏á derived features ‡πÅ‡∏•‡∏∞ business metrics
- ‡∏ó‡∏≥ statistical analysis ‡πÅ‡∏•‡∏∞ hypothesis testing
- ‡∏û‡∏±‡∏í‡∏ô‡∏≤ KPIs ‡πÅ‡∏•‡∏∞ performance metrics

### üìä **Morning Session (3 hours)**

#### **Hour 1: Data Quality Assessment**
```python
# 2.1 ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
def assess_data_quality(df):
    quality_report = pd.DataFrame({
        'Column': df.columns,
        'Missing_Count': df.isnull().sum(),
        'Missing_Percentage': (df.isnull().sum() / len(df)) * 100,
        'Unique_Values': df.nunique()
    })
    return quality_report.sort_values('Missing_Percentage', ascending=False)

quality_report = assess_data_quality(df)
print("=== DATA QUALITY REPORT ===")
print(quality_report.head(20))
```

#### **Hour 2: Data Cleaning**
```python
# 2.2 ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
def clean_lending_data(df):
    df_clean = df.copy()
    
    # Convert percentage columns
    if 'int_rate' in df_clean.columns:
        df_clean['int_rate'] = pd.to_numeric(
            df_clean['int_rate'].astype(str).str.rstrip('%'), 
            errors='coerce'
        )
    
    # Create binary target
    df_clean['is_bad_loan'] = (df_clean['loan_status'] == 'Charged Off').astype(int)
    
    return df_clean

df_clean = clean_lending_data(df)
print(f"Shape after cleaning: {df_clean.shape}")
```

#### **Hour 3: Feature Engineering**
```python
# 2.3 ‡∏™‡∏£‡πâ‡∏≤‡∏á Features ‡πÉ‡∏´‡∏°‡πà
def create_derived_features(df):
    df_features = df.copy()
    
    # FICO Score average
    if 'fico_range_low' in df.columns and 'fico_range_high' in df.columns:
        df_features['fico_avg'] = (df['fico_range_low'] + df['fico_range_high']) / 2
    
    # Income to loan ratio
    df_features['income_to_loan_ratio'] = df['annual_inc'] / df['loan_amnt']
    
    # Grade to numeric
    grade_mapping = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}
    df_features['grade_numeric'] = df['grade'].map(grade_mapping)
    
    return df_features

df_features = create_derived_features(df_clean)
```

### üïê **Afternoon Session (3 hours)**

#### **Hour 4: Statistical Analysis**
```python
# 2.4 ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
from scipy import stats

# T-test: FICO scores between good and bad loans
good_loans = df_features[df_features['is_bad_loan'] == 0]['fico_avg'].dropna()
bad_loans = df_features[df_features['is_bad_loan'] == 1]['fico_avg'].dropna()

t_stat, p_value = stats.ttest_ind(good_loans, bad_loans)
print(f"FICO T-test: t-statistic = {t_stat:.2f}, p-value = {p_value:.4f}")

# Chi-square test: Grade vs Default
contingency_table = pd.crosstab(df_features['grade'], df_features['is_bad_loan'])
chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)
print(f"Grade Chi-square: statistic = {chi2:.2f}, p-value = {p_value:.4f}")
```

#### **Hour 5-6: KPI Development & ML Preparation**
```python
# 2.5 KPIs ‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ML
business_kpis = {
    'total_loans': len(df_features),
    'total_amount': df_features['loan_amnt'].sum(),
    'avg_loan_size': df_features['loan_amnt'].mean(),
    'overall_default_rate': df_features['is_bad_loan'].mean()
}

print("=== BUSINESS KPIs ===")
for kpi, value in business_kpis.items():
    print(f"{kpi}: {value:,.2f}")

# Prepare ML dataset
ml_features = [
    'loan_amnt', 'int_rate', 'annual_inc', 'fico_avg',
    'grade_numeric', 'income_to_loan_ratio'
]

X = df_features[ml_features].fillna(df_features[ml_features].median())
y = df_features['is_bad_loan']
```

### üìù **Day 2 Deliverables**
1. **Data Quality Report**: ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç
2. **Feature Engineering Summary**: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ features ‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•
3. **Statistical Analysis Report**: ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
4. **Clean Dataset**: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ML

---

## ü§ñ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 3: Predictive Analytics & ML
*‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Machine Learning ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ*

### üéØ **Learning Objectives**
- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• classification ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö credit risk
- ‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á business recommendations
- ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏£‡∏∞‡∏ö‡∏ö scoring ‡πÅ‡∏•‡∏∞ decision framework

### üìä **Morning Session (3 hours)**

#### **Hour 1: Model Building**
```python
# 3.1 ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Credit Risk
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, classification_report

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Train model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Evaluate
y_pred_proba = rf_model.predict_proba(X_test)[:, 1]
auc_score = roc_auc_score(y_test, y_pred_proba)

print(f"Model AUC Score: {auc_score:.3f}")
```

#### **Hour 2: Feature Importance & Model Interpretation**
```python
# 3.2 ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Feature Importance
feature_importance = pd.DataFrame({
    'feature': ml_features,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

print("=== TOP RISK FACTORS ===")
print(feature_importance.head())
```

#### **Hour 3: Business Application**
```python
# 3.3 ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö Credit Scoring
def create_scoring_system(model, threshold=0.25):
    def score_application(application_data):
        features = pd.DataFrame([application_data])[ml_features]
        risk_probability = model.predict_proba(features)[0, 1]
        
        if risk_probability <= 0.15:
            decision = "APPROVE"
            rate_tier = "Best"
        elif risk_probability <= 0.25:
            decision = "APPROVE"
            rate_tier = "Standard"
        else:
            decision = "DECLINE"
            rate_tier = "N/A"
        
        return {
            'risk_score': int(risk_probability * 1000),
            'decision': decision,
            'rate_tier': rate_tier
        }
    
    return score_application

scoring_function = create_scoring_system(rf_model)
```

### üïê **Afternoon Session (3 hours)**

#### **Hour 4-6: Customer Segmentation & Implementation**
```python
# 3.4 Customer Segmentation
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Prepare customer features
customer_features = df_features.groupby('member_id').agg({
    'loan_amnt': 'sum',
    'annual_inc': 'first',
    'fico_avg': 'first',
    'is_bad_loan': 'max'
}).fillna(0)

# Clustering
scaler = StandardScaler()
X_cluster = scaler.fit_transform(customer_features)

kmeans = KMeans(n_clusters=4, random_state=42)
customer_features['segment'] = kmeans.fit_predict(X_cluster)

print("=== CUSTOMER SEGMENTS ===")
print(customer_features.groupby('segment').mean())
```

### üìù **Day 3 Deliverables**
1. **Trained ML Model**: ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£ optimize ‡πÅ‡∏•‡πâ‡∏ß
2. **Scoring System**: ‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà
3. **Customer Segmentation**: ‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå
4. **Implementation Guide**: ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á

---

## üîó ‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ß‡∏±‡∏ô

### üìà **Progressive Learning Path**

**Day 1 ‚Üí Day 2**:
- ‡πÉ‡∏ä‡πâ insights ‡∏à‡∏≤‡∏Å descriptive analysis ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
- Business understanding ‡∏ô‡∏≥‡πÑ‡∏õ‡∏™‡∏π‡πà‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á relevant features
- Data quality issues ‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡πÅ‡∏£‡∏Å‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á

**Day 2 ‚Üí Day 3**:
- Clean dataset ‡πÅ‡∏•‡∏∞ engineered features ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
- Statistical insights ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å features ‡πÅ‡∏•‡∏∞‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
- KPIs ‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ evaluate business impact

### üéØ **Skill Building Progression**

| Day | Technical Skills | Business Skills | Tools |
|-----|-----------------|----------------|-------|
| 1 | pandas, matplotlib, basic stats | Domain understanding, data interpretation | Jupyter, pandas |
| 2 | scipy, feature engineering, hypothesis testing | KPI development, statistical thinking | pandas, scipy, seaborn |
| 3 | scikit-learn, model evaluation | Business impact assessment, decision making | sklearn, deployment tools |

---

## üí° Tips ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡∏™‡∏≠‡∏ô

### üìö **Teaching Strategies**

#### **‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Day 1**
- **Start with Business**: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å business problem ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà technical details
- **Interactive Exploration**: ‡πÉ‡∏´‡πâ‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏™‡∏≥‡∏£‡∏ß‡∏à‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏ä‡∏£‡πå findings
- **Visual First**: ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ visualizations ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£ insights
- **Context Building**: ‡πÄ‡∏•‡πà‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á Lending Club ‡πÉ‡∏´‡πâ‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à

#### **‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Day 2**
- **Systematic Approach**: ‡∏™‡∏≠‡∏ô‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö
- **Statistical Thinking**: ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏™‡∏π‡∏ï‡∏£
- **Business Relevance**: ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á features ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß‡∏Å‡∏±‡∏ö business logic
- **Quality Focus**: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ô‡∏¥‡∏™‡∏±‡∏¢‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

#### **‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Day 3**
- **Model Understanding**: ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
- **Business Translation**: ‡∏ù‡∏∂‡∏Å‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå technical ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à
- **Implementation Reality**: ‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á
- **Ethical Considerations**: ‡∏´‡∏≤‡∏£‡∏∑‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á bias ‡πÅ‡∏•‡∏∞ fairness

### üîß **Technical Preparations**

#### **Environment Setup**
```python
# Required packages
packages = [
    'pandas>=1.3.0',
    'numpy>=1.21.0', 
    'matplotlib>=3.4.0',
    'seaborn>=0.11.0',
    'scikit-learn>=1.0.0',
    'scipy>=1.7.0'
]
```

#### **Common Issues & Solutions**

**Memory Issues**:
```python
# For large datasets
chunk_size = 10000
for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    process_chunk(chunk)
```

**Data Loading Problems**:
```python
# Robust data loading
try:
    df = pd.read_csv('data.csv', encoding='utf-8')
except UnicodeDecodeError:
    df = pd.read_csv('data.csv', encoding='latin-1')
```

### üéØ **Assessment Methods**

#### **Day 1 Assessment**
- **Practical Exercise** (30 minutes): Data exploration challenge
- **Mini-Presentation** (10 minutes): Present one key insight

#### **Day 2 Assessment**
- **Data Cleaning Challenge** (45 minutes): Clean a messy dataset
- **Statistical Analysis** (30 minutes): Perform and interpret tests

#### **Day 3 Assessment**
- **Model Building Project** (60 minutes): Build and evaluate ML model
- **Business Case** (30 minutes): Translate model results to business recommendations

### üìä **Engagement Techniques**

1. **Real-world Relevance**: ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
2. **Interactive Polling**: ‡πÉ‡∏ä‡πâ polls ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° predictions ‡∏Å‡πà‡∏≠‡∏ô‡∏î‡∏π‡∏ú‡∏•‡∏à‡∏£‡∏¥‡∏á
3. **Group Competitions**: ‡πÅ‡∏Ç‡πà‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏≤ insights ‡∏´‡∏£‡∏∑‡∏≠ build model ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
4. **Case Study Discussions**: ‡∏≠‡∏†‡∏¥‡∏õ‡∏£‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à
5. **Error Learning**: ‡πÉ‡∏ä‡πâ common mistakes ‡πÄ‡∏õ‡πá‡∏ô teaching moments

### ‚úÖ **Learning Objectives Checklist**

**End of Day 1**:
- [ ] ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ load ‡πÅ‡∏•‡∏∞ explore ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á basic visualizations
- [ ] ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à business context
- [ ] ‡∏£‡∏∞‡∏ö‡∏∏ patterns ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô

**End of Day 2**:
- [ ] ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏£‡∏∞‡∏ö‡∏ö
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á derived features
- [ ] ‡πÉ‡∏ä‡πâ statistical tests ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
- [ ] ‡∏û‡∏±‡∏í‡∏ô‡∏≤ business KPIs

**End of Day 3**:
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô ML models
- [ ] ‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö scoring
- [ ] ‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£ implement

---

*üìñ ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏ô‡∏µ‡πâ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà*

---
[‚¨ÖÔ∏è ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ó‡∏µ‡πà Case Studies](./06_case_studies.md) | [‚û°Ô∏è ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Exercises](./08_exercises.md)